<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Zijun Yang </title> <meta name="author" content="Zijun Yang"> <meta name="description" content="my &lt;a href='https://scholar.google.com/citations?user=yble580AAAAJ&amp;hl=en' style='text-decoration: underline;'&gt;google scholar&lt;/a&gt; profile&lt;br&gt;&lt;br&gt; ^ denotes equal contribution "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zijunyangeo.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zijun Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">my <a href="https://scholar.google.com/citations?user=yble580AAAAJ&amp;hl=en" style="text-decoration: underline;" rel="external nofollow noopener" target="_blank">google scholar</a> profile<br><br> ^ denotes equal contribution </p> </header> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> </div> <div class="abbr2"> <abbr2 class="badge rounded w-100" style="background-color:#006666; color: white;"> <div style="color: white;">working paper</div> </abbr2> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> under review </year> </div> </div> <div id="working1" class="col-sm-8"> <div class="title">EMET: An emergence-based thermal phenological framework for near real-time crop type mapping</div> <div class="author"> <em>Zijun Yang</em>, Chunyuan Diao, Feng Gao, and Bo Li </div> <div class="periodical"> <em>working paper</em>, under review </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Near real-time (NRT) crop type mapping plays a crucial role in modeling crop development, managing food supply chains, and supporting sustainable agriculture. Yet NRT crop type mapping is challenging due to the obstacle in acquiring timely crop type reference labels during the current season for crop mapping model building. Meanwhile, the crop mapping models constructed with historical crop type labels and corresponding satellite imagery may not be applicable to the current season in NRT due to spatiotemporal variability of crop phenology. The difficulty in characterizing crop phenology in NRT remains a significant hurdle in NRT crop type mapping. To tackle these issues, a novel emergence-based thermal phenological framework (EMET) is proposed in this study for field-level NRT crop type mapping. The EMET framework comprises three key components: hybrid deep learning spatiotemporal image fusion, NRT thermal-based crop phenology normalization, and NRT crop type characterization. The EMET framework paves the way for large-scale satellite-based NRT crop type mapping at the field level, which can largely help reduce food market volatility to enhance food security, as well as benefit a variety of agricultural applications to optimize crop management towards more sustainable agricultural production. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> </div> <div class="abbr2"> <abbr2 class="badge rounded w-100" style="background-color:#006666; color: white;"> <div style="color: white;">working paper</div> </abbr2> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> under review </year> </div> </div> <div id="working2" class="col-sm-8"> <div class="title">Multi-stream STGAN: A Spatiotemporal Image Fusion Model with Improved Temporal Transferability</div> <div class="author"> Fangzheng Lyu^, <em>Zijun Yang^</em>, Chunyuan Diao, and Shaowen Wang </div> <div class="periodical"> <em>working paper</em>, under review </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Spatiotemporal satellite image fusion aims to generate remote sensing images satisfying both high spatial and temporal resolution by integrating different satellite imagery datasets with distinct spatial and temporal resolutions. Such fusion technique is crucial for numerous applications that require frequent monitoring at fine spatial and temporal scales spanning agriculture, environment, natural resources and disaster management. However, existing fusion models have difficulty accommodating abrupt spatial changes in land cover among images and dealing with temporally distant image data. This study proposes a novel multi-stream spatiotemporal fusion generative adversarial network (STGAN) model for spatiotemporal satellite image fusion that can produce accurate fused images and accommodate substantial temporal differences between the input images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#003366; color: white;"> <div style="color: white;">journal article</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2023 </year> </div> </div> <div id="RN364" class="col-sm-8"> <div class="title">CropSow: An integrative remotely sensed crop modeling framework for field-level crop planting date estimation</div> <div class="author"> Yin Liu, Chunyuan Diao, and <em>Zijun Yang</em> </div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Crop planting timing is critical in regulating environmental conditions of crop growth throughout the season, and is an essential parameter in crop simulation models for estimating dry matter accumulation and yields. Accurate planting date information is key to characterizing crop growing dynamics under varying farming practices and facilitating agricultural adaptation to climate change. To date, the main methods to acquire planting dates include field survey methods, weather-dependent methods, and remote sensing phenological detecting methods. However, it is still challenging to effectively estimate the crop planting dates at field levels due to the lack of appropriate field-level modeling design as well as the dearth of ground planting reference data. In our study, we develop a novel CropSow modeling framework to estimate field-level planting dates by integrating the remote sensing phenological detecting method with the crop growth model. The remote sensing phenological detecting method is devised to retrieve the critical crop phenological metrics of farm fields from remote sensing time series, which are then integrated into the crop growth model for field planting date estimation in consideration of soil-crop-atmosphere continuum. CropSow leverages the rich physiological knowledge embedded in the crop growth model to scalably interpret satellite observations under a variety of environmental and management conditions for field-level planting date retrievals. With corn in Illinois, US as a case study, the developed CropSow outperforms three advanced benchmark models (i.e., the remote sensing accumulative growing degree day method, the weather-dependent method, and the shape model) in crop planting date estimation at the field level, with R square higher than 0.68, root mean square error (RMSE) lower than 10 days, and mean bias error (MBE) around 5 days from 2016 to 2020. It achieves better generalization performance than the benchmark models, as well as stronger adaptability to abnormal weather conditions with more robust performance in estimating the planting dates of farm fields. CropSow holds considerable promise to extrapolate over space and time for estimating the timing of crop planting of individual farm fields at large scales.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN364</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yin and Diao, Chunyuan and Yang, Zijun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CropSow: An integrative remotely sensed crop modeling framework for field-level crop planting date estimation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Journal of Photogrammetry and Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{334-355}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0924-2716}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#003366; color: white;"> <div style="color: white;">journal article</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2023 </year> </div> </div> <div id="RN296" class="col-sm-8"> <div class="title">Towards Scalable Within-Season Crop Mapping with Phenology Normalization and Deep Learning</div> <div class="author"> <em>Zijun Yang</em>, Chunyuan Diao, and Feng Gao </div> <div class="periodical"> <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Crop-type mapping using time-series remote sensing data is crucial for a wide range of agricultural applications. Crop mapping during the growing season is particularly critical in timely monitoring of the agricultural system. Most existing studies focusing on within-season crop mapping leverage historical remote sensing and crop type reference data for model building, due to the difficulty in obtaining timely crop type samples for the current growing season. Yet the crop type samples from previous years may not be used directly considering the diverse patterns of crop phenology across years and locations, which hampers the scalability and transferability of the model to the current season for timely crop mapping. This article proposes an innovative within-season emergence (WISE) phenology normalized deep learning model towards scalable within-season crop mapping. The crop time-series remote sensing data are first normalized by the WISE crop emergence dates before being fed into an attention-based one-dimensional convolutional neural network classifier. Compared to conventional calendar-based approaches, the WISE-phenology normalization approach substantially helps the deep learning crop mapping model accommodate the spatiotemporal variations in crop phenological dynamics. Results in Illinois from 2017 to 2020 indicate that the proposed model outperforms calendar-based approaches and yields over 90% overall accuracy for classifying corn and soybeans at the end of season. During the growing season, the proposed model can give satisfactory performance (85% overall accuracy) one to four weeks earlier than calendar-based approaches. With WISE-phenology normalization, the proposed model exhibits more stable performance across Illinois and can be transferred to different years with enhanced scalability and robustness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN296</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Zijun and Diao, Chunyuan and Gao, Feng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Scalable Within-Season Crop Mapping with Phenology Normalization and Deep Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1939-1404}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#003366; color: white;"> <div style="color: white;">journal article</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2023 </year> </div> </div> <div id="RN362" class="col-sm-8"> <div class="title">Monitoring spring leaf phenology of individual trees in a temperate forest fragment with multi-scale satellite time series</div> <div class="author"> Yilun Zhao, Chunyuan Diao, Carol K Augspurger, and <em>Zijun Yang</em> </div> <div class="periodical"> <em>Remote Sensing of Environment</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Forest fragmentation has been increasingly exacerbated by deforestation, urbanization, and agricultural expansion. Monitoring the forest fragments via the lens of tree-crown scale leaf phenology is critical to understand tree species phenological responses to climate change and identify the fragment species vulnerable to environmental disturbance. Despite advances in remote sensing for phenology monitoring, detecting tree-crown scale leaf phenology in fragmented forests remains challenging. Simultaneous tracking of key spring phenological events that are crucial to ecosystem functions and climate change responses is also neglected. To address these challenges, we develop a novel tree-crown scale remote sensing phenological monitoring framework to characterize all the critical spring phenological events of individual trees of deciduous forest fragments, with Trelease Woods in Champaign, Illinois as a case study. The novel framework comprises four components: 1) generate high spatiotemporal resolution fusion imagery from multi-scale satellite time series with a hybrid deep learning fusion model; 2) calibrate PlanetScope imagery time series with fusion data using histogram matching; 3) model tree-crown scale phenology trajectory with a Beck logistic-based method; 4) detect a diversity of tree-crown scale phenological events using several phenological metric extraction methods (i.e., threshold- and curve feature-based methods). Combined with weekly in-situ phenological observations of 123 individual trees across 12 broadleaf species from 2017 to 2020, the framework effectively bridges the satellite- and field-based phenological measures for the key spring phenological events (i.e., budswell, budburst, leaf expansion, and leaf maturity events) at the tree-crown scale, particularly for large individuals (RMSE &lt;1 week for most events). Calibration of PlanetScope imagery using multi-scale satellite fusion data in consideration of landscape fragmentation is critical for monitoring tree phenology of forest fragments. Compared to curve feature-based methods, threshold-based phenometric extraction methods demonstrate enhanced capability in detecting spring leaf phenological dynamics of individual trees. Among the phenological events, full leaf out and early leaf expansion events are retrieved with high accuracy using calibrated PlanetScope time series (RMSE from 3 to 5 days and R-squared higher than 0.8). With both intensive satellite and field phenological efforts, this novel framework is at the forefront of interpreting tree-crown scale remotely sensed phenological metrics in the context of biologically meaningful field phenological events in fragmented forest setting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN362</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Yilun and Diao, Chunyuan and Augspurger, Carol K and Yang, Zijun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Monitoring spring leaf phenology of individual trees in a temperate forest fragment with multi-scale satellite time series}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Remote Sensing of Environment}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{297}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{113790}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0034-4257}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#af7f1b; color: white;"> <div style="color: white;">conference paper</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2022 </year> </div> </div> <div id="RN361" class="col-sm-8"> <div class="title">CyberGIS for Scalable Remote Sensing Data Fusion</div> <div class="author"> Fangzheng Lyu, <em>Zijun Yang</em>, Zimo Xiao, Chunyuan Diao, Jinwoo Park, and Shaowen Wang </div> <div class="periodical"> <em>Practice and Experience in Advanced Research Computing</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Satellite remote sensing data products are widely used in many applications and science domains ranging from agriculture and emergency management to Earth and environmental sciences. Researchers have developed sophisticated and computationally intensive models for processing and analyzing such data with varying spatiotemporal resolutions from multiple sources. However, the computational intensity and expertise in using advanced cyberinfrastructure have held back the scalability and reproducibility of such models. To tackle this challenge, this research employs the CyberGIS-Compute middleware to achieve scalable and reproducible remote sensing data fusion across multiple spatiotemporal resolutions by harnessing advanced cyberinfrastructure. CyberGIS-Compute is a cyberGIS middleware framework for conducting computationally intensive geospatial analytics with advanced cyberinfrastructure resources such as those provisioned by XSEDE. Our case study achieved remote sensing data fusion at high spatial and temporal resolutions based on integrating CyberGIS-Compute with a cutting-edge deep learning model. This integrated approach also demonstrates how to achieve computational reproducibility of scalable remote sensing data fusion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN361</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lyu, Fangzheng and Yang, Zijun and Xiao, Zimo and Diao, Chunyuan and Park, Jinwoo and Wang, Shaowen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CyberGIS for Scalable Remote Sensing Data Fusion}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Practice and Experience in Advanced Research Computing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#003366; color: white;"> <div style="color: white;">journal article</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2021 </year> </div> </div> <div id="RN251" class="col-sm-8"> <div class="title">Hybrid phenology matching model for robust crop phenological retrieval</div> <div class="author"> Chunyuan Diao, <em>Zijun Yang</em>, Feng Gao, Xiaoyang Zhang, and Zhengwei Yang </div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Crop phenology regulates seasonal agroecosystem carbon, water, and energy exchanges, and is a key component in empirical and process-based crop models for simulating biogeochemical cycles of farmlands, assessing gross and net primary production, and forecasting the crop yield. The advances in phenology matching models provide a feasible means to monitor crop phenological progress using remote sensing observations, with a priori information of reference shapes and reference phenological transition dates. Yet the underlying geometrical scaling assumption of models, together with the challenge in defining phenological references, hinders the applicability of phenology matching in crop phenological studies. The objective of this study is to develop a novel hybrid phenology matching model to robustly retrieve a diverse spectrum of crop phenological stages using satellite time series. The devised hybrid model leverages the complementary strengths of phenometric extraction methods and phenology matching models. It relaxes the geometrical scaling assumption and can characterize key phenological stages of crop cycles, ranging from farming practice-relevant stages (e.g., planted and harvested) to crop development stages (e.g., emerged and mature). To systematically evaluate the influence of phenological references on phenology matching, four representative phenological reference scenarios under varying levels of phenological calibrations in terms of time and space are further designed with publicly accessible phenological information. The results indicate that the hybrid phenology matching model can achieve high accuracies for estimating corn and soybean phenological growth stages in Illinois, particularly with the year- and region-adjusted phenological reference (R-squared higher than 0.9 and RMSE less than 5 days for most phenological stages). The inter-annual and regional phenological patterns characterized by the hybrid model correspond well with those in the crop progress reports (CPRs) from the USDA National Agricultural Statistics Service (NASS). Compared to the benchmark phenology matching model, the hybrid model is more robust to the decreasing levels of phenological reference calibrations, and is particularly advantageous in retrieving crop early phenological stages (e.g., planted and emerged stages) when the phenological reference information is limited. This innovative hybrid phenology matching model, together with CPR-enabled phenological reference calibrations, holds considerable promise in revealing spatio-temporal patterns of crop phenology over extended geographical regions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN251</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Diao, Chunyuan and Yang, Zijun and Gao, Feng and Zhang, Xiaoyang and Yang, Zhengwei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hybrid phenology matching model for robust crop phenological retrieval}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Journal of Photogrammetry and Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{181}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{308-326}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0924-2716}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2"> <div class="abbr"> <abbr class="badge rounded w-100" style="background-color:#003366; color: white;"> <div style="color: white;">journal article</div> </abbr> </div> <div class="abbr2"> </div> <div class="year"> <year class="badge rounded w-100" style="color: white;"> 2021 </year> </div> </div> <div id="RN248" class="col-sm-8"> <div class="title">A Robust Hybrid Deep Learning Model for Spatiotemporal Image Fusion</div> <div class="author"> <em>Zijun Yang</em>, Chunyuan Diao, and Bo Li </div> <div class="periodical"> <em>Remote Sensing</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Dense time-series remote sensing data with detailed spatial information are highly desired for the monitoring of dynamic earth systems. Due to the sensor tradeoff, most remote sensing systems cannot provide images with both high spatial and temporal resolutions. Spatiotemporal image fusion models provide a feasible solution to generate such a type of satellite imagery, yet existing fusion methods are limited in predicting rapid and/or transient phenological changes. Additionally, a systematic approach to assessing and understanding how varying levels of temporal phenological changes affect fusion results is lacking in spatiotemporal fusion research. The objective of this study is to develop an innovative hybrid deep learning model that can effectively and robustly fuse the satellite imagery of various spatial and temporal resolutions. The proposed model integrates two types of network models: super-resolution convolutional neural network (SRCNN) and long short-term memory (LSTM). SRCNN can enhance the coarse images by restoring degraded spatial details, while LSTM can learn and extract the temporal changing patterns from the time-series images. To systematically assess the effects of varying levels of phenological changes, we identify image phenological transition dates and design three temporal phenological change scenarios representing rapid, moderate, and minimal phenological changes. The hybrid deep learning model, alongside three benchmark fusion models, is assessed in different scenarios of phenological changes. Results indicate the hybrid deep learning model yields significantly better results when rapid or moderate phenological changes are present. It holds great potential in generating high-quality time-series datasets of both high spatial and temporal resolutions, which can further benefit terrestrial system dynamic studies. The innovative approach to understanding phenological changes’ effect will help us better comprehend the strengths and weaknesses of current and future fusion models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RN248</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Robust Hybrid Deep Learning Model for Spatiotemporal Image Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Zijun and Diao, Chunyuan and Li, Bo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5005}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zijun Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>